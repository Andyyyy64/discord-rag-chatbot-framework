import { GoogleGenerativeAI } from '@google/generative-ai';

import { loadEnv } from '../../config/env';
import { embedQuery } from '../../infrastructure/gemini/embedding-service';
import { logger } from '../../infrastructure/logging/logger';
import { getSupabaseClient } from '../../infrastructure/supabase/client';
import { createBaseError } from '../../shared/errors/base-error';
import { createRerankService } from '../common/rerank-service';
import type { RerankResult } from '../common/rerank-service';

import type { ChatAnswer, ChatCommandInput } from './types';

interface MessageWindowRecord {
  window_id: string;
  text: string | null;
  message_ids: string[];
  start_at: string;
  end_at: string;
  channel_id: string;
  guild_id: string;
}

/**
 * ãƒãƒ£ãƒƒãƒˆã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆã™ã‚‹
 */
export function createChatService(rerankService = createRerankService()) {
  const supabase = getSupabaseClient();
  const env = loadEnv();
  const rerankTopK = Math.max(1, env.RERANK_TOPK ?? 5);
  const model = new GoogleGenerativeAI(env.GEMINI_API_KEY).getGenerativeModel({
    model: env.CHAT_MODEL,
    generationConfig: {
      temperature: 0.3,
      topP: 0.9,
      maxOutputTokens: 2048,
    },
  });

  /**
   * ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦RAGãƒ™ãƒ¼ã‚¹ã®å›ç­”ã‚’ç”Ÿæˆã™ã‚‹
   */
  const answer = async (input: ChatCommandInput): Promise<ChatAnswer> => {
    const started = Date.now();
    logger.info(`[Chat] ğŸ’¬ New chat request from user ${input.userId}: "${input.query}"`);
    
    const windows = await fetchCandidateWindowsHybrid(input);

    if (!windows.length) {
      logger.warn('[Chat] âš ï¸ No windows found, sync may be required');
      return {
        answer: 'ã¾ã åŒæœŸã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚/sync ã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚',
        citations: [],
        latencyMs: Date.now() - started,
      };
    }

    logger.info(`[Chat] ğŸ“‹ Found ${windows.length} candidate windows, selecting best for prompt...`);
    const selectedWindows = await selectWindowsForPrompt(input, windows);
    logger.info(`[Chat] âœ… Selected ${selectedWindows.length} windows for generation`);
    
    const prompt = buildPrompt(input, selectedWindows);
    const promptTokens = Math.ceil(prompt.length / 4); // æ¦‚ç®—
    logger.info(`[Chat] ğŸ“ Prompt built (~${promptTokens} tokens)`);
    
    try {
      logger.info(`[Chat] ğŸ¤– Calling Gemini ${env.CHAT_MODEL}...`);
      const genStart = Date.now();
      const response = await model.generateContent({
        contents: [
          {
            role: 'user',
            parts: [{ text: prompt }],
          },
        ],
      });

      logger.info(`[Chat] âœ… Gemini response received (${Date.now() - genStart}ms)`);

      const text = response.response?.candidates?.[0]?.content?.parts
        ?.map((part) => part.text ?? '')
        .join('')
        .trim();

      const answerLength = text?.length ?? 0;
      logger.info(`[Chat] ğŸ“¤ Answer generated (${answerLength} chars, ${Date.now() - started}ms total)`);

      return {
        answer: text?.length ? text : 'å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚',
        citations: buildCitations(input, selectedWindows),
        latencyMs: Date.now() - started,
      };
    } catch (error) {
      logger.error('[Chat] âŒ Gemini chat failed', error);
      throw createBaseError('ãƒãƒ£ãƒƒãƒˆå¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ', 'CHAT_FAILED');
    }
  };

  /**
   * ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆpgvector RPCï¼‰ã‚’ä¸»è»¸ã«å€™è£œã‚’å–å¾—ã™ã‚‹
   */
  const fetchCandidateWindowsHybrid = async (
    input: ChatCommandInput
  ): Promise<MessageWindowRecord[]> => {
    const searchStart = Date.now();
    logger.info(`[Chat] ğŸ” Starting vector search for query: "${input.query}"`);
    
    try {
      // ã‚¹ãƒ†ãƒƒãƒ— 1: ã‚¯ã‚¨ãƒªã® embedding ã‚’ç”Ÿæˆ
      const embeddingStart = Date.now();
      const queryEmbedding = await embedQuery(input.query, 3072);
      logger.info(
        `[Chat] âœ… Query embedding generated (${Date.now() - embeddingStart}ms, ${queryEmbedding.length} dimensions)`
      );

      // ã‚¹ãƒ†ãƒƒãƒ— 2: pgvector ã® RPC ã§ã‚®ãƒ«ãƒ‰å†… Top-K ã‚’å–å¾—
      const vectorStart = Date.now();
      const VECTOR_LIMIT = 200; // å¾Œæ®µã§ã•ã‚‰ã«Top-Nã«çµã‚‹
      const { data: matched, error: matchError } = await supabase.rpc(
        'match_windows_in_guild',
        {
          query_embedding: queryEmbedding,
          p_guild_id: input.guildId,
          p_limit: VECTOR_LIMIT,
        }
      );

      if (matchError) {
        logger.error('[Chat] âŒ Vector RPC error:', matchError);
        return [];
    }

      logger.info(
        `[Chat] ğŸ“Š Vector RPC complete (${Date.now() - vectorStart}ms): ${matched?.length ?? 0} candidates`
      );

      if (!matched || matched.length === 0) {
        logger.warn('[Chat] âš ï¸ No vector matches');
        return [];
      }

      // ã‚¹ãƒ†ãƒƒãƒ— 3: window æƒ…å ±ã‚’å–å¾—ã—ã€é¡ä¼¼åº¦é †ã«æ•´åˆ—ï¼ˆä¸Šä½15ä»¶ï¼‰
      const windowIds = matched.map((m: { window_id: string }) => m.window_id);
      const { data: windows, error: windowError } = await supabase
        .from('message_windows')
        .select('window_id,text,message_ids,start_at,end_at,channel_id,guild_id')
        .in('window_id', windowIds);

      if (windowError) {
        logger.error('[Chat] âŒ Window fetch error:', windowError);
        return [];
      }

      const byId = new Map(windows?.map((w) => [w.window_id, w]) ?? []);
      const ordered = matched
        .map((m: { window_id: string; similarity: number }) => byId.get(m.window_id))
        .filter((w): w is MessageWindowRecord => Boolean(w))
        .slice(0, 15);
      
      logger.info(
        `[Chat] âœ¨ Vector search complete (${Date.now() - searchStart}ms total), returning top ${ordered.length}`
      );
      return ordered;
    } catch (error) {
      logger.error('[Chat] Vector search failed', error);
      return [];
    }
  };

  /**
   * ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹
   */
  const buildPrompt = (input: ChatCommandInput, windows: MessageWindowRecord[]): string => {
    const context = windows
      .map((w, index) => `[#${index + 1}] (${w.start_at} â€“ ${w.end_at})\n${w.text ?? '(å†…å®¹ãªã—)'}`)
      .join('\n\n');

    return [
      'ã‚ãªãŸã¯Discordã‚µãƒ¼ãƒãƒ¼å°‚ç”¨ã®RAGã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚',
      'ä»¥ä¸‹ã®åˆ¶ç´„ã‚’å¿…ãšå®ˆã£ã¦ãã ã•ã„:',
      '1. å‚ç…§ã—ãŸè¨¼æ‹ ã«ã¯ [#ç•ªå·] ã®å½¢ã§æ ¹æ‹ ç•ªå·ã‚’ä»˜ã‘ã‚‹ã€‚',
      '2. å›ç­”ã¯æ—¥æœ¬èªã‚’æ—¢å®šã¨ã—ã€å¿…è¦ã«å¿œã˜ã¦è‹±èªã‚’æ··åœ¨ã—ã¦ã‚‚ã‚ˆã„ã€‚',
      '3. æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ç‡ç›´ã«ä¸è¶³ã‚’ä¼ãˆã‚‹ã€‚',
      '',
      '# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ',
      context,
      '',
      `# ãƒ¦ãƒ¼ã‚¶ãƒ¼ (${input.userId}) ã‹ã‚‰ã®è³ªå•`,
      input.query,
    ].join('\n');
  };

  /**
   * å¼•ç”¨æƒ…å ±ã‚’æ§‹ç¯‰ã™ã‚‹
   */
  const buildCitations = (input: ChatCommandInput, windows: MessageWindowRecord[]) =>
    windows.slice(0, 3).map((window, index) => ({
      label: `[#${index + 1}] ${new Date(window.start_at).toLocaleString('ja-JP')}`,
      jumpLink: `https://discord.com/channels/${input.guildId}/${input.channelId}/${window.message_ids?.[0] ?? ''}`,
    }));

  /**
   * ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ä½¿ç”¨ã™ã‚‹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ãƒªãƒ©ãƒ³ã‚¯ã—ã¦é¸æŠã™ã‚‹
   */
  const selectWindowsForPrompt = async (
    input: ChatCommandInput,
    windows: MessageWindowRecord[]
  ): Promise<MessageWindowRecord[]> => {
    // Rerank ã‚µãƒ¼ãƒ“ã‚¹ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ãƒªãƒ©ãƒ³ã‚¯
    if (env.RERANK_PROVIDER !== 'none') {
    const candidates = windows.map((window, index) => ({
      id: window.window_id,
      content: window.text ?? '',
      meta: window,
      score: windows.length - index,
    }));

    const reranked = await rerankService.rerank(input.query, candidates, rerankTopK);
      if (reranked.length > 0) {
    return reranked
      .map((result: RerankResult) => result.meta as MessageWindowRecord)
      .filter((window): window is MessageWindowRecord => Boolean(window));
      }
    }

    // ãƒªãƒ©ãƒ³ã‚¯ã—ãªã„ã€ã¾ãŸã¯å¤±æ•—ã—ãŸå ´åˆã¯ãã®ã¾ã¾è¿”ã™
    return windows.slice(0, rerankTopK);
  };

  return { answer };
}
